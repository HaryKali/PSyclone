In this section of the tutorial you will see how to parallelise code
with OpenMP directives using the LFRic API.

## Example

We will be using the same example that was used in the distributed
memory part of the tutorial.  This example is extracted from the LFRic
model and is one of the most computationally costly sections of the
LFRic dynamical core. Note, that all of the executable code has been
removed apart from the part we are interested in, the invoke call.

It is probably worth reminding yourself of the content of the invoke call:

    call invoke( setval_c(grad_p, 0.0_r_def),                                &
                 scaled_matrix_vector_kernel_type(grad_p, p, div_star,       &
                                                  hb_inv),                   &
                 enforce_bc_kernel_type( grad_p ),                           &
                 apply_variable_hx_kernel_type(                              &
                       Hp, grad_p, mt_lumped_inv, p,                         &
                       compound_div, p3theta, ptheta2, m3_exner_star,        &
                       tau_t, timestep_term) )

setval_c is a builtin that iterates over dofs,
scaled_matrix_vector_kernel_type and enforce_variable_hx_kernel_type
are references to kernels that can modify kernels that are continuous
or discontinuous as specified in their kernel metadata (and therefore
it must be assumed that the modified field is continuous) and
apply_variable_hx_kernel_type is a reference to a kernel that modifies
a discontinuous field as specified in its kernel metadata.

## Add OpenMP parallel loop directives

From this directory run

> psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py ../code/helmholtz_solver_alg_mod.x90

From the psy-layer PSyIR that is output to the screen you should see
that only two of the four loops have been parallelised.

Above the PSyIR output you should see some lines explaining why two of
the loops were not parallelised. This information is output from the
transformation that is used in the omp_script.py script:

Transformation Error: Error in DynamoOMPParallelLoopTrans transformation. The kernel has an argument with INC access. Colouring is required.

If you take a look at omp_script.py you will see that we
try to apply OpenMP parallelisation to all loops but if the
transformation does not let us do this and raises an
exception we print out the exception text.

> my_favourite_editor omp_script.py

While you are looking at this script you will also see that rather
than using the walk method to iterate over all nodes of type loop as
we have done in the previous tutorial section (schedule.walk(Loop)) we
use a shortcut method built in to the node to do this instead
(schedule.loops()).

# Mixed mode MPI and OpenMP code

Before moving on it is worth mentioning that you have just generating
mixed mode (MPI and OpenMP) parallel code here which many manually
parallelised codes do not support. If you just want to generate OpenMP
code you can switch off the distributed memory generation option as
you did in the previous tutorial section (i.e. add the -nodm options
to the command line). If you like you could reduce the number of halo
exchanges generated by setting the ANNEXED_DOFS switch to True in the
configuration file, again as you did in the previous tutorial
section. The point here is that these options can be used
interchangeably with OpenMP parallelisation.

## Colouring

The error messages produced by the OpenMP transformation hint at why
it did not parallelise two of the loops. The reason is that these
loops interate over cells and update shared dofs. This means that
different iterations of the loop can contribute to the same dof value
which means the iterations of the loop are not independent of each
other.

There are various solutions to this problem. The one typically taken
in finite element codes is to colour the loop. Colouring should
hopefully have already been explained to you so we will not go into
much detail here. Essentially loop iterations are grouped together into
sets that are independent of each other. Each of these groups is
called a colour. Iterations with the same colour can be run in
parallel but iterations with different colours can not. Therefore
colouring splits a loop into two loops, the outmost one is sequential
and iterates over each of the colours, the inner one is parallel and
iterates over all iterations with the same colour.

Let's colour the required loops. We want to colour a loop if it
modifies fields that are, or might be, continuous. The simplest way to
do this is to perform the inverse check i.e. colour if any modified
fields in the loop are not discontinuous.

Add the following code next to the OpenMP parallel loop transformation creation (otrans)

ctrans = Dynamo0p3ColourTrans()

and add the following within the invoke for loop after schedule is declared

        for loop in schedule.loops():
            if loop.field_space.orig_name not in FunctionSpace.VALID_DISCONTINUOUS_NAMES:
                ctrans.apply(loop)

What happens when we run this?

The reason we get an exception in the colouring transformation is
because the first loop containing the builtin may modify a continuous
function but it iterates over dofs and therefore does not need to be
coloured (as all accesses to dofs are independent). The colouring
transformation checks for this and therefore raises an exception.

Let's fix this problem by adding another clause to the if test to make
sure we only colour loops which iterate over cells ...

        for loop in schedule.walk(Loop):
            if loop.field_space.orig_name not in FunctionSpace.VALID_DISCONTINUOUS_NAMES \
               and loop.iteration_space == "cell_column":
                ctrans.apply(loop)

This is looking better. You should see the previously unparallelised
loops are now split into two and the inner one (iterations over a
single colour) is parallelised using OpenMP.

Notice that you have not had to change the OpenMP transformation
logic, it has just done the right thing. This is because the script
tries to parallelise all loops and the OpenMP transformation refuses
to parallelise a loop that it knows to be serial. It knows this as
PSyclone has specified that this loop is of type "colours". You can
see the loop types in the psy-layer PSyIR code that is output to the
screen.

If you take a look above the psy-layer PSyIR code that is output to
the screen you will see messages from the transformation saying that
it will not parallelise a loop that iterates over colours:

Transformation Error: Error in DynamoOMPParallelLoopTrans transformation. The requested loop is over colours and must be computed serially.

## Generated code

So far we have only looked at the PSyIR. Let's now take a look at the generated code

> my_fav_edtitor psy.f90

Notice that the OpenMP parallel directives have clauses specifying
what is shared and what is private. Normally the HPC expert would have
to work these out and check them when the code was modified. PSyclone
works them out and adds them automatically.

The OpenMP parallel directives also default to a static schedule. The
OpenMP schedule can be changed when creating the transformation
object. Valid values are:

['runtime', 'static', 'dynamic', 'guided', 'auto']

Try setting the value and see if the generated code changes, e.g.:

    otrans = DynamoOMPParallelLoopTrans(omp_schedule="dynamic")

Feel free to try to provide an invalid value, you should get an
exception.

Take a look a the generated coloured loops. They call infrastructure
functions to provide the bounds and the dofmaps now have "cmap"
lookups in them. These changes are all taken care of by
PSyclone. However, note that PSyclone is not responsible for working
out the number of colours, the colour map etc, PSyclone relies on the
LFRic infrastructure to provide this information and simply asks for
it via the LFRic infrastructure's API.

## Reductions

The example we have been using does not make use of
reductions. Therefore lets modify the example slightly to add one
in. This has already been done in the helmholtz_solver_alg_mod.x90
algorithm file in this directory. The only difference to the original
example is that an inner produce builtin has been added at the end of
the invoke call.

From this directory run:

> psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py helmholtz_solver_alg_mod.x90 -d ../code

The -d option tells PSyclone where to look for the kernel code. You
can try without it if you like. PSyclone will complain that it can't
find one of the required files.

The psy-layer PSyIR the is output to the screen is showing that the innerproduct builtin has been parallelised. Take a look at the generaetd code:

> my_fav_editor psy.f90

Notice that the OpenMP directive around the inner product loop has a
reduction clause. PSyclone has determined that a reduction is required
(from the kernel metadata) and automatically added the appropriate
OpenMP clause. Therefore the user does not need to worry about this.

## Reproducible reductions

According to the OpenMP specification an OpenMP reduction does not
need to guarantee the order in which the operations in a reduction
takes place. Therefore an OpenMP reduction might return different
results from one run to the next due to rounding errors when summing
values in different orders.

This can cause problems for correctness checking and debugging. In
fact the ability to get reproducible results (via a switch in the
code) is actually a requirement in the current Met Office model.

PSyclone therefore provides an option to compute reductions in a
reproducible way. This implementation requires the use of OMP PARALLEL
and OMP LOOP directives rather than the single OMP PARALLEL LOOP
directive that we have been using so far.

Let's modify the script. Create the following two new transformations
next to the existing ones:

    ptrans = OMPParallelTrans()
    ltrans = Dynamo0p3OMPLoopTrans()

Replace the following line in the script:

                otrans.apply(loop)

with

                if loop.reductions():
                    ptrans.apply(loop)
		    ltrans.apply(loop)
                else:
                    otrans.apply(loop)

The reductions() method in a loop node is handy here!

If you rerun:

> psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py helmholtz_solver_alg_mod.x90 -d ../code

You should see the psy-layer PSyIR that is output to the screen having an OMP parallel and an OMP do directive around the last loop. You should also see the reprod=False clause.

Take a look at the generated code

> my_fav_editor psy.f90.

The generated code is very similar to what we were generating before,
it just now has two directives around the reduction loop rather than
one.

Now we can specify that we would like a reproducible reduction

In the script replace:

                    ltrans.apply(loop)

with

                    ltrans.apply(loop, {"reprod": True})

The PSyIR that is output to the screen should now specify that reprod=True for the OMP DO directive.

Take a look at the generated code:

> my_fav_editor psy.f90.

In this version the values for each thread are summed separately in
the parallel loop and then these partial sums are then added together
serially at the end. This is a more complicated solution but PSyclone
takes care of it without the user needing to worry.

If you look at the allocation of the l_sum array, you will see that it
is padded by 8. This is to avoid false sharing between the threads. If
these value needs to be changed it can be modified in the
configuration file. A copy of the configuration file is provided in
this directory if you would like to try this. Modify, the
REPROD_PAD_SIZE to whatever value you think appropriate. You can
select to use this configuration file by adding --config psyclone.cfg
to the pscylone command i.e.

> psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py helmholtz_solver_alg_mod.x90 -d ../code --config psyclone.cfg


Notice that you could also set REPRODUCIBLE_REDUCTIONS to true in the
config file rather than setting it in the script. This would make all
reductions reproducible by default, rather than selectively doing it
in the script. You can try this out if you like by removing the reprod
option in the script, changing the config file and re-running.

## Finished

Well done, you have finished the LFRic OpenMP part of the tutorial