In this example you will see how to parallelise code with OpenMP
directives using the LFRic API.

## Assumptions

???

## Example

We will be using the same example that was used in the distributed
memory part of the tutorial.  This example is extracted from the LFRic
model and is one of the most computationally costly sections of the
LFRic dynamical core. Note, that all of the executable code has been
removed apart from the part we are interested in, the invoke call.

## Add OpenMP parallel loop directives

From this directory run

> psyclone -oalg /dev/null -opsy psy.f90 -s ./omp_script.py ../code/helmholtz_solver_alg_mod.x90

From the psy-layer PSyIR that is output to the screen you should see
that only two of the four loops have been parallelised.

Above the PSyIR output you should see some lines explaining why two of
the loops were not parallelised. This information is output from the
transformation that is used in the omp_script.py script:

Transformation Error: Error in DynamoOMPParallelLoopTrans transformation. The kernel has an argument with INC access. Colouring is required.

If you take a look at the script omp_script.py you will see that we
try to apply OpenMP parallelisation to all loops but the
transformation does not let us do this is certain cases, raises an
exception and we print this exception out.

> my_favourite_editor omp_script.py

While we are here you can see that rather than using the walk method
to iterate over all nodes of type loop as we have done in the previous
tutorial section (schedule.walk(Loop)) we use a shortcut method built
in to the node to do this instead.

# Mixed mode MPI and OpenMP code

Before moving on it is worth mentioning that you are generating mixed
mode (MPI and OpenMP) parallel code here which many manually
parallelised codes do not support. If you just want to generate OpenMP
code you can switch off the distributed memory generation option as
you did in the previous tutorial section (i.e. add the -nodm options
to the command line). If you like you could reduce the number of halo
exchanges generated by setting the ANNEXED_DOFS switch to True in the
configuration file, again as you did in the previous tutorial
section. The point here is that these options can be used
interchangeably with OpenMP parallelisation.

## Colouring

The error messages produced by the OpenMP transformation hint at why
it did not parallelise two of the loops. The reason is that these
loops interate over cells and update shared dofs. This means that
different iterations of the loop can contribute to the same dof value
which means the iterations of the loop are not independent of each
other.

There are various solutions to this problem. The one typically taken
in finite element codes is to colour the loop. Colouring should
hopefully have already been explained to you so we will not go into
much detail here. Essentially iterations are grouped together into
sets that are independent of each other. Each of these groups is
called a colour. Iterations with the same colour can be run in
parallel but iterations with different colours can not. Therefore
colouring splits a loop into two loops, the outmost one is sequential
and iterates over each of the colours, the inner one is parallel and
iterates over all iterations with the same colour.

Let's colour the required loops. We want to colour a loop if it
modifies fields that are, or might be, continuous. The simplest way to
do this is to perform the inverse check i.e. colour if any modified
fields in the loop are not discontinuous.

Add the following code next to the OpenMP parallel loop transformation creation (otrans)

ctrans = Dynamo0p3ColourTrans()

and add the following within the invoke for loop after schedule is declared

        for loop in schedule.loops():
            if loop.field_space.orig_name not in FunctionSpace.VALID_DISCONTINUOUS_NAMES:
                ctrans.apply(loop)

What happens when we run this?

The reason we get an exception in the colouring transformation is
because the first loop may modify a continuous function but it
iterates over dofs and therefore does not need to be coloured (as all
accesses to dofs are independent). The colouring transformation knows
this and therefore complains when its validation method is called.

Let's fix this problem by adding another clause to the if test to make
sure we only colour loops which iterate over cells ...

        for loop in schedule.walk(Loop):
            if loop.field_space.orig_name not in FunctionSpace.VALID_DISCONTINUOUS_NAMES \
               and loop.iteration_space == "cell_column":
                ctrans.apply(loop)

This is looking better. You should see the previously unparallelised
loops are now split into two and the inner one (iterations over a
single colour) is parallelised using OpenMP.

Notice that you have not had to change the OpenMP transformation
logic, it has just done the right thing. This is because the script
tries to parallelise all loops and the OpenMP transformation refuses
to parallelise a loop that it knows to be serial. It knows this as the
loop class has a property which can be used to identify the type of
loop it is.

## Generated code

So far we have only looked at the PSyIR. Let's now take a look at the generated code

> my_fav_edtitor psy.f90

Notice that the OpenMP parallel directives have clauses on them
specifying what is shared and what is private. Normally the HPC expert
would have to work these out and check them when the code was
modified. PSyclone works them out and adds them automatically.

The OpenMP parallel directives also default to a static schedule. The
OpenMP schedule can be chaged in the transformation arguments. Try:

******

Also notice that the coloured loops have loop bounds that calls
infrastructure to provide the bounds and that the dofmaps now have
"cmap" lookups in them. These changes are all taken care of by
PSyclone. However, PSyclone is not responsible for working out the
number of colours, the colour map etc, PSyclone relies on the LFRic
infrastructure to provide this information and simply asks for it via
the LFRic infrastructure's API.

## Reductions

Reduction has been parallelised. However is not guaranteed to provide the same results from one run to the next as spec says can sum up in any order.

Can modify transformation to produce reproducible global sums ...

Now we sum to private values and add the thread results in a sequential order, rather than using the reduction. Notice padding added to avoid false sharing. Size of padding can be configured for particular architecture.

*** GET ERROR ON MASTER ***

## Finished

Well done, you have finished the LFRic OpenMP part of the tutorial